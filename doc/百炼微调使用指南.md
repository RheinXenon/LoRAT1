# 阿里云百炼平台微调使用指南

## 一、数据准备

### 1.1 转换数据格式

你的 MedQA 数据集需要转换为百炼平台要求的 SFT 格式。我已为你创建了转换脚本。

#### 单个文件转换
```bash
python convert_to_bailian_format.py datasets/MedQA/questions/Mainland/4_options/train.jsonl datasets/MedQA_BaiLian/mainland_train.jsonl
```

#### 批量转换所有数据集
```bash
python batch_convert.py
```

这会将所有 MedQA 数据集转换并保存到 `datasets/MedQA_BaiLian/` 目录。

### 1.2 数据格式说明

**原始格式 (MedQA):**
```json
{
    "question": "卧位腰椎穿刺，脑脊液压力正常值是（　　）。",
    "options": {
        "A": "80～180mmH2O（0.78～1.76kPa）",
        "B": "50～70mmH2O（0.49～0.69kPa）",
        "C": "230～250mmH2O（2.25～2.45kPa）",
        "D": "260～280mmH2O（2.55～2.74kPa）"
    },
    "answer": "80～180mmH2O（0.78～1.76kPa）",
    "answer_idx": "A"
}
```

**百炼 SFT 格式:**
```json
{
    "messages": [
        {
            "role": "system",
            "content": "你是一个专业的医学助手，擅长回答医学选择题。请根据题目和选项，给出正确答案。"
        },
        {
            "role": "user",
            "content": "卧位腰椎穿刺，脑脊液压力正常值是（　　）。\n\n选项：\nA. 80～180mmH2O（0.78～1.76kPa）\nB. 50～70mmH2O（0.49～0.69kPa）\nC. 230～250mmH2O（2.25～2.45kPa）\nD. 260～280mmH2O（2.55～2.74kPa）"
        },
        {
            "role": "assistant",
            "content": "答案是 A. 80～180mmH2O（0.78～1.76kPa）"
        }
    ]
}
```

## 二、百炼平台操作步骤

### 2.1 前提条件

1. ✅ 已开通阿里云百炼服务（北京地域）
2. ✅ 已获取 API-KEY
3. ✅ 已安装 DashScope SDK（如使用命令行）

```bash
pip install dashscope
```

### 2.2 配置 API Key

**Windows PowerShell:**
```powershell
$env:DASHSCOPE_API_KEY="your-api-key-here"
```

**Windows CMD:**
```cmd
set DASHSCOPE_API_KEY=your-api-key-here
```

**Linux/Mac:**
```bash
export DASHSCOPE_API_KEY="your-api-key-here"
```

### 2.3 上传训练文件

#### 方式一：使用命令行（推荐）

```bash
# 上传训练集
dashscope files.upload -f "datasets/MedQA_BaiLian/mainland_4opt_train.jsonl" -p fine_tune -d "MedQA Mainland training dataset"

# 上传验证集（可选）
dashscope files.upload -f "datasets/MedQA_BaiLian/mainland_4opt_dev.jsonl" -p fine_tune -d "MedQA Mainland validation dataset"
```

上传成功后会返回 `file_id`，请记录下来：
```
Upload success, file id: 976bd01a-f30b-4414-86fd-50c54486e3ef
```

#### 方式二：使用 HTTP API

```bash
curl --location --request POST \
'https://dashscope.aliyuncs.com/api/v1/files' \
--header 'Authorization: Bearer '${DASHSCOPE_API_KEY} \
--form 'files=@"datasets/MedQA_BaiLian/mainland_4opt_train.jsonl"' \
--form 'purpose="fine-tune"' \
--form 'descriptions="MedQA training dataset"'
```

**重要限制：**
- 单个文件最大 300MB
- 总存储空间配额 5GB
- 最多 100 个有效文件

### 2.4 创建微调任务

#### 使用命令行

```bash
dashscope fine_tunes.call \
  -m qwen2.5-7b-instruct \
  -t '<训练集file_id>' \
  -v '<验证集file_id>' \
  --mode sft \
  -b 16 \
  -e 3 \
  -l 1e-5 \
  --hyper_parameters split=0.9 warmup_ratio=0.05 eval_steps=50 max_length=2048
```

#### 使用 HTTP API

```bash
curl --location 'https://dashscope.aliyuncs.com/api/v1/fine-tunes' \
--header 'Authorization: Bearer '${DASHSCOPE_API_KEY} \
--header 'Content-Type: application/json' \
--data '{
    "model": "qwen2.5-7b-instruct",
    "training_file_ids": ["<训练集file_id>"],
    "validation_file_ids": ["<验证集file_id>"],
    "hyper_parameters": {
        "n_epochs": 3,
        "batch_size": 16,
        "learning_rate": "1e-5",
        "split": 0.9,
        "warmup_ratio": 0.05,
        "eval_steps": 50,
        "max_length": 2048
    },
    "training_type": "sft"
}'
```

## 三、推荐配置

### 3.1 模型选择

| 模型 | 代码 | 适用场景 |
|------|------|----------|
| 通义千问 2.5-7B | `qwen2.5-7b-instruct` | 平衡性能和成本 ⭐推荐 |
| 通义千问 2.5-14B | `qwen2.5-14b-instruct` | 更好的效果 |
| 通义千问 2.5-32B | `qwen2.5-32b-instruct` | 最佳效果（成本较高） |
| 通义千问-Turbo | `qwen-turbo-0624` | 商业版，适合生产环境 |

### 3.2 超参数推荐

#### 基础配置（数据量 < 10,000）
```json
{
    "n_epochs": 3,              // 循环次数：3-5 次
    "batch_size": 16,           // 批次大小：使用默认值
    "learning_rate": "1e-5",    // 学习率：SFT 推荐 1e-5
    "split": 0.9,               // 训练/验证集分割比例
    "warmup_ratio": 0.05,       // 学习率预热比例
    "eval_steps": 50,           // 验证间隔步数
    "max_length": 2048          // 最大序列长度（建议 8192）
}
```

#### 大数据量配置（数据量 > 10,000）
```json
{
    "n_epochs": 1,              // 循环次数：1-2 次
    "batch_size": 16,
    "learning_rate": "1e-5",
    "split": 0.9,
    "warmup_ratio": 0.05,
    "eval_steps": 100,
    "max_length": 4096
}
```

#### 高效训练（LoRA）配置
```json
{
    "training_type": "efficient_sft",
    "n_epochs": 3,
    "batch_size": 16,
    "learning_rate": "1e-4",    // efficient_sft 推荐 1e-4
    "lora_rank": 64,            // LoRA 秩值：推荐 64
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "target_modules": "ALL",    // 推荐 ALL
    "max_length": 2048
}
```

### 3.3 训练类型选择

| 训练类型 | 代码 | 说明 | 成本 | 效果 |
|----------|------|------|------|------|
| SFT 全参训练 | `sft` | 调整所有参数 | 高 | 最好 |
| SFT 高效训练 | `efficient_sft` | LoRA 训练 | 低 | 好 ⭐推荐 |
| DPO 全参训练 | `dpo_full` | 偏好对齐 | 高 | - |
| DPO 高效训练 | `dpo_lora` | LoRA + 偏好 | 低 | - |
| 继续预训练 | `cpt` | 领域知识注入 | 很高 | - |

**对于医学选择题任务，推荐使用 `efficient_sft`（LoRA 微调）。**

## 四、计费说明

### 计费公式
```
训练费用 = (训练数据 Token 总数 + 混合训练数据 Token 总数) × 循环次数 × 训练单价
```

### 成本估算示例

假设：
- 训练数据：10,000 条
- 平均每条：200 tokens
- 循环次数：3 次
- 训练单价：约 0.002 元/千 token（具体以官网为准）

```
总 Token 数 = 10,000 × 200 = 2,000,000 tokens
训练费用 = 2,000,000 × 3 × 0.002 / 1000 = 12 元
```

**注意：** 模型微调后需要部署才能使用，部署费用另计。

## 五、完整流程示例

### Step 1: 转换数据
```bash
python batch_convert.py
```

### Step 2: 配置环境
```bash
pip install dashscope
export DASHSCOPE_API_KEY="your-api-key"
```

### Step 3: 上传训练文件
```bash
# 上传训练集
dashscope files.upload -f "datasets/MedQA_BaiLian/mainland_4opt_train.jsonl" -p fine_tune -d "训练集"
# 记录返回的 file_id: train_file_id

# 上传验证集
dashscope files.upload -f "datasets/MedQA_BaiLian/mainland_4opt_dev.jsonl" -p fine_tune -d "验证集"
# 记录返回的 file_id: val_file_id
```

### Step 4: 创建微调任务（推荐 LoRA）
```bash
dashscope fine_tunes.call \
  -m qwen2.5-7b-instruct \
  -t '<train_file_id>' \
  -v '<val_file_id>' \
  --mode efficient_sft \
  -b 16 \
  -e 3 \
  -l 1e-4 \
  --hyper_parameters "lora_rank=64 target_modules=ALL max_length=2048"
```

### Step 5: 查看训练状态
```bash
# 使用命令行查看
dashscope fine_tunes.get -j '<job_id>'

# 或访问控制台
# https://bailian.console.aliyun.com/
```

### Step 6: 训练完成后部署模型

在控制台中找到训练完成的模型，点击"部署"按钮。

### Step 7: 调用微调后的模型

```python
from dashscope import Generation

response = Generation.call(
    model='<your_fine_tuned_model_id>',
    messages=[{
        'role': 'system',
        'content': '你是一个专业的医学助手。'
    }, {
        'role': 'user',
        'content': '卧位腰椎穿刺，脑脊液压力正常值是？\n\nA. 80～180mmH2O\nB. 50～70mmH2O\nC. 230～250mmH2O\nD. 260～280mmH2O'
    }]
)
print(response.output.text)
```

## 六、常见问题

### Q1: 文件超过 300MB 怎么办？
A: 可以将数据分割成多个文件。在创建微调任务时，`training_file_ids` 可以接受多个文件 ID。

```bash
# 上传多个文件
dashscope files.upload -f "train_part1.jsonl" -p fine_tune
dashscope files.upload -f "train_part2.jsonl" -p fine_tune

# 创建任务时指定多个文件
dashscope fine_tunes.call -m qwen2.5-7b-instruct -t '<file_id1>' '<file_id2>' --mode sft
```

### Q2: 应该选择 SFT 还是 efficient_sft？
A: 
- **efficient_sft (LoRA)**: 推荐首选，训练快、成本低、效果好
- **sft (全参)**: 数据量大（>50k）且需要最佳效果时使用

### Q3: 如何评估微调效果？
A: 
1. 在控制台查看训练曲线（loss、accuracy）
2. 使用验证集评估
3. 部署后用测试集实际测试

### Q4: 训练失败怎么办？
A: 常见原因：
- 数据格式不正确（检查 JSON 格式）
- 超参数设置不当（使用推荐配置）
- 序列长度超限（调整 max_length）

## 七、参考资料

- [百炼平台官方文档](https://help.aliyun.com/zh/model-studio/)
- [模型微调 API 文档](https://help.aliyun.com/zh/model-studio/developer-reference/fine-tune-api)
- [DashScope SDK 文档](https://help.aliyun.com/zh/model-studio/developer-reference/sdk-overview)

## 八、技术支持

如遇到问题，可以：
1. 查看百炼平台控制台的详细日志
2. 访问阿里云工单系统
3. 查阅官方文档和社区论坛

