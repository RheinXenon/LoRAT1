# 🚀 快速开始 - 阿里云百炼模型微调

## 📋 前提检查

在开始之前，请确保：

- ✅ 已开通阿里云百炼服务（北京地域）
- ✅ 已在 `.env` 文件中配置了 `DASHSCOPE_API_KEY`
- ✅ 已转换数据格式（运行过 `batch_convert.py`）

## 🎯 完整流程（推荐）

### 方式一：使用自动化脚本（最简单）

```bash
# 1. 安装依赖
pip install dashscope python-dotenv

# 2. 运行自动化脚本
cd 数据处理
python fine_tune_automation.py --auto
```

脚本会自动完成：
1. ✅ 列出可用数据集
2. ✅ 上传训练和验证文件
3. ✅ 创建微调任务
4. ✅ 监控训练进度
5. ✅ 自动更新 `.env` 文件

### 方式二：分步骤执行

#### Step 1: 转换数据格式

```bash
# 如果还没转换，先运行
cd 数据处理
python batch_convert.py
```

转换后的文件在 `datasets/MedQA_BaiLian/` 目录。

#### Step 2: 上传训练文件

```bash
# 方式 A: 使用自动化脚本
python fine_tune_automation.py --upload

# 方式 B: 使用命令行
dashscope files.upload -f "../datasets/MedQA_BaiLian/mainland_4opt_train.jsonl" -p fine_tune -d "训练集"
dashscope files.upload -f "../datasets/MedQA_BaiLian/mainland_4opt_dev.jsonl" -p fine_tune -d "验证集"
```

**重要：** 记录返回的 `file_id`，并更新到 `.env` 文件：
```
TRAIN_FILE_ID=your_train_file_id
VALIDATION_FILE_ID=your_validation_file_id
```

#### Step 3: 创建微调任务

```bash
# 方式 A: 使用自动化脚本
python fine_tune_automation.py --create

# 方式 B: 使用命令行（LoRA 训练）
dashscope fine_tunes.call \
  -m qwen2.5-7b-instruct \
  -t '<训练集file_id>' \
  -v '<验证集file_id>' \
  --mode efficient_sft \
  -b 16 \
  -e 3 \
  -l 1e-4 \
  --hyper_parameters "lora_rank=64 target_modules=ALL max_length=2048"
```

**重要：** 记录返回的 `job_id`，并更新到 `.env` 文件：
```
FINE_TUNE_JOB_ID=your_job_id
```

#### Step 4: 监控训练进度

```bash
# 方式 A: 使用自动化脚本（实时监控）
python fine_tune_automation.py --monitor <job_id>

# 方式 B: 使用命令行（手动查询）
dashscope fine_tunes.get -j '<job_id>'

# 方式 C: 在控制台查看
# https://bailian.console.aliyun.com/
```

#### Step 5: 部署模型

训练完成后，在百炼控制台：
1. 找到训练完成的模型
2. 点击"部署"按钮
3. 等待部署完成（几分钟）
4. 获取模型 ID

**重要：** 将模型 ID 更新到 `.env` 文件：
```
FINE_TUNED_MODEL_ID=your_model_id
```

#### Step 6: 测试微调模型

```bash
# 使用自动化脚本测试
python fine_tune_automation.py --test <model_id>
```

## 📊 .env 配置示例

完成所有步骤后，你的 `.env` 文件应该包含：

```bash
# 基础配置
DASHSCOPE_API_KEY="sk-xxxxxx"

# 微调配置
FINE_TUNE_BASE_MODEL=qwen2.5-7b-instruct
TRAIN_FILE_ID=file-xxxxx
VALIDATION_FILE_ID=file-yyyyy
FINE_TUNE_JOB_ID=ft-xxxxx
FINE_TUNED_MODEL_ID=qwen-xxxxx

# 超参数（可选，使用默认值）
TRAINING_TYPE=efficient_sft
N_EPOCHS=3
BATCH_SIZE=16
LEARNING_RATE=1e-4
```

## 🎨 自动化脚本功能

`fine_tune_automation.py` 支持多种操作模式：

```bash
# 交互式菜单（推荐新手）
python fine_tune_automation.py

# 自动执行完整流程
python fine_tune_automation.py --auto

# 仅上传文件
python fine_tune_automation.py --upload

# 仅创建任务
python fine_tune_automation.py --create

# 查询任务状态（单次）
python fine_tune_automation.py --status <job_id>

# 监控任务进度（持续）
python fine_tune_automation.py --monitor <job_id>

# 测试模型
python fine_tune_automation.py --test <model_id>
```

## 📝 常用命令速查

### 文件管理

```bash
# 列出所有文件
dashscope files.list

# 查看文件详情
dashscope files.get -f <file_id>

# 删除文件
dashscope files.delete -f <file_id>
```

### 任务管理

```bash
# 列出所有任务
dashscope fine_tunes.list

# 查看任务详情
dashscope fine_tunes.get -j <job_id>

# 取消任务
dashscope fine_tunes.cancel -j <job_id>
```

## ⚙️ 推荐配置

### 小数据集（< 10,000 条）

```bash
FINE_TUNE_BASE_MODEL=qwen2.5-7b-instruct
TRAINING_TYPE=efficient_sft
N_EPOCHS=3
LEARNING_RATE=1e-4
LORA_RANK=64
MAX_LENGTH=2048
```

### 大数据集（> 10,000 条）

```bash
FINE_TUNE_BASE_MODEL=qwen2.5-14b-instruct
TRAINING_TYPE=efficient_sft
N_EPOCHS=1
LEARNING_RATE=1e-4
LORA_RANK=64
MAX_LENGTH=4096
```

### 高质量（成本较高）

```bash
FINE_TUNE_BASE_MODEL=qwen2.5-32b-instruct
TRAINING_TYPE=sft
N_EPOCHS=2
LEARNING_RATE=1e-5
MAX_LENGTH=4096
```

## 💰 成本估算

以 mainland_4opt_train.jsonl（约 10,000 条数据）为例：

```
数据量: 10,000 条
平均长度: ~200 tokens/条
循环次数: 3
总 tokens: 10,000 × 200 × 3 = 6,000,000

预估费用（efficient_sft）:
- 训练费用: ~12 元
- 部署费用: 按调用量计费
```

**注意：** 实际费用以百炼平台显示为准。

## ❓ 常见问题

### Q1: 文件上传失败？

**原因：** 文件超过 300MB 或格式错误

**解决：**
1. 检查文件大小（单文件限制 300MB）
2. 验证 JSON 格式是否正确
3. 如果文件太大，可以分割成多个文件

### Q2: 训练任务失败？

**原因：** 数据格式不正确或超参数设置不当

**解决：**
1. 检查数据格式是否符合 SFT 格式要求
2. 使用推荐的超参数配置
3. 查看控制台的详细错误日志

### Q3: 如何评估模型效果？

**方法：**
1. 在控制台查看训练曲线（Loss、Accuracy）
2. 使用测试集评估准确率
3. 人工测试实际问答效果

### Q4: 训练需要多长时间？

**参考时间：**
- 1,000 条数据（LoRA）：10-30 分钟
- 10,000 条数据（LoRA）：1-3 小时
- 10,000 条数据（全参）：3-6 小时

## 🔗 相关资源

- [详细使用指南](./百炼微调使用指南.md)
- [百炼控制台](https://bailian.console.aliyun.com/)
- [百炼官方文档](https://help.aliyun.com/zh/model-studio/)
- [API 文档](https://help.aliyun.com/zh/model-studio/developer-reference/fine-tune-api)

## 💡 小提示

1. **首次微调建议：** 使用小数据集（1000-2000条）快速测试
2. **模型选择：** 推荐从 `qwen2.5-7b-instruct` + `efficient_sft` 开始
3. **数据质量：** 高质量的 1000 条数据比低质量的 10000 条更有效
4. **验证集：** 建议单独准备验证集，有助于监控过拟合
5. **成本控制：** 使用 LoRA（efficient_sft）可以大幅降低成本

## 🎉 完成！

恭喜你完成了模型微调！现在你可以：

1. ✅ 在应用中调用微调后的模型
2. ✅ 使用测试集评估模型效果
3. ✅ 根据效果调整超参数重新训练
4. ✅ 部署到生产环境

有问题？查看 [百炼微调使用指南.md](./百炼微调使用指南.md) 获取更详细的说明。

